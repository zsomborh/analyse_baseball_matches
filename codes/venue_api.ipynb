{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Baseball game Data\n",
    "## Zsombor Hegedűs & Brúnó Helmeczy\n",
    "\n",
    "#### Prepared for: Coding 3: Data Analysis & Management with Python \n",
    "#### Instructor: Eszter Somos \n",
    "#### MSc Business Analytics @ Central European University\n",
    "#### [Github](https://github.com/zsomborh/analyse_baseball_matches) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Notebook to get weather data\n",
    "\n",
    "The purpose of this notebook is to create a dataframe which has weather data for given venue in given date. This is done in three major parts:\n",
    "\n",
    "1. Create geolocations and get longitude and latitude data for given baseball venue\n",
    "2. Use VisualCrossing API to get weather data for all matches between the 2017 and 2018 seasion in given location in given day. \n",
    "3. Merge and clean these dataframes so that they can be used for further analysis\n",
    "\n",
    "**Disclaimer** - this notebook can not be reproduced fully without an API key for VisualCrossing. Our API key is loaded in one of the markdown cells, but we don't distribute this as we have a paid subscription to the site. If anyone wishes to reproduce the notebook we highly recommend creating a free account at [VisualCrossing](https://www.visualcrossing.com/weather/weather-data-services#/login) - it will allow 100 observations to be fetched for free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import requests as re \n",
    "\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Getting geolocations based on venue\n",
    "\n",
    "We load up a preprocessed dataframe, and get all locational data with the use of geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Please set up the data folder as your default directory !!!\n",
    "os.chdir(r'C:\\Users\\T450s\\Desktop\\programming\\git\\analyse_baseball_matches\\data')\n",
    "\n",
    "# load data from disk\n",
    "\n",
    "df = pd.read_csv('Baseball_Merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialise an object called gecooder using the combination of RateLimiter and Nominatim (both are from the geopy package)\n",
    "# these will be used to get geolocations based on data that we have, which is the name of the baseball pitch\n",
    "\n",
    "geocoder = RateLimiter(Nominatim(user_agent='ba').geocode, min_delay_seconds=1)\n",
    "\n",
    "# We will get the unique venues so that we can apply geocoder to those\n",
    "venues = df['venue_name'].unique()\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for i in venues: \n",
    "    location_dict[i] = geocoder(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoder is very powerful but it is not working 100% given that we only feed in the name of a baseball pitch\n",
    "    # some of which might not even exist anymore\n",
    "\n",
    "# We will identify potentially erroneous locations by looking at the type of the observation - if this is not a stadium\n",
    "# we do a manual inspection \n",
    "    \n",
    "error_pairs = {}\n",
    "for venue in venues:\n",
    "    try:\n",
    "        if location_dict[venue].raw['type'] != 'stadium':\n",
    "            error_pairs[venue] = location_dict[venue].raw['type']\n",
    "    except:\n",
    "        error_pairs[venue] = 'None'\n",
    "\n",
    "# Searching on the net we found the correct names for stadiums that were erroneously mapped.\n",
    "# We will do a new geolocation search for the new names later\n",
    "\n",
    "\n",
    "rename_dict = {'Miller Park':'American Family Field',\n",
    "'O.co Coliseum': 'RingCentral Coliseum',\n",
    "'U.S. Cellular Field' : 'Guaranteed Rate Field',\n",
    "'AT&T Park' : 'Oracle Park',\n",
    "'Oakland Coliseum' : 'RingCentral Coliseum',\n",
    "'Fort Bragg Field' : 'Fort Bragg Field baseball',\n",
    "'Williamsport Little League Classic' : 'Williamsport, Pennsylvania',\n",
    "'Angel Stadium': 'Angel Stadium of Anaheim',\n",
    "'BB&T Ballpark': 'Williamsport, Pennsylvania'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take the new dictionary, which is rename dict, and find every geolocatoin that correspond to the new names\n",
    "# after that we append these new names to the already existing location_dict\n",
    "\n",
    "for k,v in rename_dict.items():\n",
    "    location_dict[v] = geocoder(v)\n",
    "    \n",
    "# After that we remove the previously found, but erroneous names\n",
    "\n",
    "for key in [k for k,v in rename_dict.items()]:\n",
    "    if key in location_dict:\n",
    "        del location_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create subdf, which is simple table that has unique combinations of dates and venues\n",
    "subdf = df[['date','venue_name']].drop_duplicates()\n",
    "\n",
    "# We map the new venue name to the dataframe we created earlier\n",
    "to_replace = [k for k,v in rename_dict.items()]\n",
    "\n",
    "new_venue = []\n",
    "\n",
    "for i in subdf['venue_name']:\n",
    "    if i in to_replace:\n",
    "        new_venue.append(rename_dict[i])\n",
    "    else:\n",
    "        new_venue.append(i)\n",
    "\n",
    "subdf['new_venue'] = new_venue\n",
    "\n",
    "# Now we add locational data and longitudes and latitudes\n",
    "subdf['location'] = [location_dict[i] for i in subdf['new_venue']]\n",
    "subdf['latitude'] = [i.latitude for i in subdf['location']]\n",
    "subdf['longitude'] = [i.longitude for i in subdf['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing resulting df to disk\n",
    "\n",
    "subdf.to_csv('date_n_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get weather data with API \n",
    "\n",
    "We will use the Visualcrossing API to get weather data for given location on given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "# We will filter out everything that did not happen in the past two years. For this we first need to convert \n",
    "# the date variable to have date datatype using the datetime package\n",
    "\n",
    "subdf['date'] = subdf.apply(lambda x : datetime.strptime(x['date'], \"%Y-%m-%d\").date(),axis= 1)\n",
    "subdf = subdf[subdf['date'] > datetime.strptime('2017-04-01','%Y-%m-%d').date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to read in the API key stored in a separate txt file on my PC\n",
    "\n",
    "f = open(r\"C:\\Users\\T450s\\Desktop\\api\\weather/visualcrossing_weather.txt\", \"r\")\n",
    "api_key = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create API link\n",
    "\n",
    "# We are using VisualCrossing Weather Data Services in order to query historic weather data.\n",
    "# We pay 0.0001$/record, so we will be extra careful when requesting data. \n",
    "# Documentation for the API is available from here: \n",
    "    # https://www.visualcrossing.com/resources/documentation/weather-data/getting-started-with-weather-data-services/\n",
    "\n",
    "# We will be using daily data and request the info to come to us as a json\n",
    "\n",
    "def keygen(latitude,longitude,hours,start_date,end_date,json_or_csv,api_key):\n",
    "    \n",
    "    BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/'\n",
    "    hist_forecast = 'history/'\n",
    "    \n",
    "    \n",
    "    locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "    aggregateHours = 'aggregateHours=' + str(hours)\n",
    "    startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "    endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "    unitGroup = 'unitGroup='+'uk'\n",
    "    contentType = 'contentType='+json_or_csv\n",
    "    dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "    dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "    key = 'key=' + str(api_key)\n",
    "    \n",
    "    \n",
    "    res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "    request_link = BaseURL+hist_forecast + res\n",
    "    return(request_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will drop a few variables because they are events happening with very low frequnency \n",
    "# (maybe some temporary fields or special competitions) - it is not worth including these in the analysis\n",
    "\n",
    "to_drop = ['Estadio de Beisbol Monterrey', 'Fort Bragg Field baseball', 'Hiram Bithorn Stadium', 'Williamsport, Pennsylvania']\n",
    "subdf = subdf[~subdf['new_venue'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to create an API dataframe the purpose of which is to provide a list of links that can be used for the API\n",
    "# We need a start date, end date, latitude, longitud information to create API links\n",
    "\n",
    "\n",
    "# create a column with the start date\n",
    "start_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'min'}).rename(columns = {'date':'start_date'}).reset_index()\n",
    "\n",
    "# create a column with end date\n",
    "end_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'max'}).rename(columns = {'date':'end_date'}).reset_index()\n",
    "\n",
    "# merge dfs \n",
    "api_df = pd.merge(start_df,end_df)\n",
    "\n",
    "# create links with custom function specified above\n",
    "api_df['links'] = api_df.apply(lambda x: keygen(x['latitude'],x['longitude'],24,x['start_date'],x['end_date'],'json',api_key) ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Please note that this cell won't run unless a valid API key is specified!!!\n",
    "\n",
    "# create empty list to append df to cache dfs resulting from interation\n",
    "weather_df = []\n",
    "\n",
    "for i, c in api_df.iterrows():\n",
    "    \n",
    "    # We first get a json from API and save it to myjson variable\n",
    "    myjson = re.get(api_df.loc[i,'links'])\n",
    "    \n",
    "    # weather_json will hold the parsed json that is now a dictionary \n",
    "    weather_json = json.loads(myjson.text)\n",
    "    \n",
    "    # get coordinates as json's structure needs such a key\n",
    "    my_coords =  [str(k) for k in [weather_json['locations'].keys()][0]][0]\n",
    "    \n",
    "    # printing to see status only\n",
    "    print(str(api_df.loc[i,'latitude']) + ',' + str(api_df.loc[i,'longitude']), 'is ready')\n",
    "    \n",
    "    # create temporary dataframe out of json\n",
    "    temp_weather_df = pd.DataFrame(weather_json['locations'][my_coords]['values'])\n",
    "    \n",
    "    # add location var so that it can be joined with sub df \n",
    "    temp_weather_df['location'] = my_coords\n",
    "\n",
    "    # append to list created in the beginning to concatenate them \n",
    "    weather_df.append(temp_weather_df)\n",
    "\n",
    "# concatenate list of dfs into one final df \n",
    "weather_df = pd.concat(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weather data to disk so that we don't have to rerun API requests - this would cost money.\n",
    "\n",
    "weather_df.to_csv('weather_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Merge resulting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up dataframes from disk\n",
    "\n",
    "weather_df = pd.read_csv('weather_df.csv',index_col = 'Unnamed: 0')\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some minor formatting to be used for joining \n",
    "\n",
    "subdf['latlong'] = subdf['latitude'].astype('str') +',' + subdf['longitude'].astype('str')\n",
    "weather_df['date'] = [datetime.fromtimestamp(date/1000).strftime('%Y-%m-%d') for date in weather_df['datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the datasets so now we will have date-location-weather data in the same dataframe\n",
    "\n",
    "merged_df = pd.merge(left = subdf, \n",
    "         right = weather_df,\n",
    "         left_on = ['date', 'latlong'],\n",
    "         right_on = ['date','location' ], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this dataframe so that it only has columns that are useful\n",
    "\n",
    "merged_df = merged_df[['date', 'venue_name', 'latitude','longitude',\n",
    "                       'wdir', 'temp', 'maxt', 'visibility', 'wspd', \n",
    "                       'cloudcover', 'mint', 'precip', 'snowdepth', \n",
    "                      'dew', 'humidity','precipcover']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>wdir</th>\n",
       "      <th>temp</th>\n",
       "      <th>maxt</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wspd</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>mint</th>\n",
       "      <th>precip</th>\n",
       "      <th>snowdepth</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipcover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Busch Stadium</td>\n",
       "      <td>38.622554</td>\n",
       "      <td>-90.193922</td>\n",
       "      <td>116.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>19.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>12.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.63</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Tropicana Field</td>\n",
       "      <td>27.768056</td>\n",
       "      <td>-82.653276</td>\n",
       "      <td>135.58</td>\n",
       "      <td>24.3</td>\n",
       "      <td>29.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>73.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Chase Field</td>\n",
       "      <td>33.445486</td>\n",
       "      <td>-112.066693</td>\n",
       "      <td>161.79</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>12.7</td>\n",
       "      <td>22.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>29.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Oakland Coliseum</td>\n",
       "      <td>37.751675</td>\n",
       "      <td>-122.199371</td>\n",
       "      <td>200.42</td>\n",
       "      <td>14.9</td>\n",
       "      <td>20.6</td>\n",
       "      <td>9.5</td>\n",
       "      <td>17.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>63.81</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Citi Field</td>\n",
       "      <td>40.757278</td>\n",
       "      <td>-73.845879</td>\n",
       "      <td>194.04</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        venue_name   latitude   longitude    wdir  temp  maxt  \\\n",
       "0  2017-04-02     Busch Stadium  38.622554  -90.193922  116.71  13.3  19.4   \n",
       "1  2017-04-02   Tropicana Field  27.768056  -82.653276  135.58  24.3  29.4   \n",
       "2  2017-04-02       Chase Field  33.445486 -112.066693  161.79  20.0  27.1   \n",
       "3  2017-04-03  Oakland Coliseum  37.751675 -122.199371  200.42  14.9  20.6   \n",
       "4  2017-04-03        Citi Field  40.757278  -73.845879  194.04  12.9  16.6   \n",
       "\n",
       "   visibility  wspd  cloudcover  mint  precip  snowdepth   dew  humidity  \\\n",
       "0         9.9  12.5        11.0   7.3    0.25        0.0   7.0     66.63   \n",
       "1         9.9  11.1         5.5  21.0    0.00        0.0  18.9     73.06   \n",
       "2         9.9  12.7        22.6  12.1    0.00        0.0   0.6     29.90   \n",
       "3         9.5  17.1        17.0   8.9    0.00        0.0   7.5     63.81   \n",
       "4         9.9  17.0        65.7   8.8    0.00        0.0   0.2     42.47   \n",
       "\n",
       "   precipcover  \n",
       "0         4.17  \n",
       "1         0.00  \n",
       "2         0.00  \n",
       "3         0.00  \n",
       "4         0.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('location_weather.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
