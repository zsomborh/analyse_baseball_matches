{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import requests as re \n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "\n",
    "df = pd.read_csv('Baseball_Merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Wrigley Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Great American Ball Park',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# We initialise a geocoder object using the combination of RateLimiter and Nominatim (both are from the geopy package)\n",
    "# these will be used to get geolocations based on data that we have, which is the name of the baseball pitch\n",
    "\n",
    "geocoder = RateLimiter(Nominatim(user_agent='ba').geocode, min_delay_seconds=1)\n",
    "\n",
    "# We will get the unique venues so that we can apply geocoder to those\n",
    "venues = df['venue_name'].unique()\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for i in venues: \n",
    "    location_dict[i] = geocoder(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoder is very powerful but it is not working 100% given that we only feed in the name of a baseball pitch\n",
    "    # some of which might not even exist anymore\n",
    "\n",
    "# We will identify potentially erroneous locations by looking at the type of the observation - if this is not a stadium\n",
    "# we do a manual inspection \n",
    "    \n",
    "error_pairs = {}\n",
    "for venue in venues:\n",
    "    try:\n",
    "        if location_dict[venue].raw['type'] != 'stadium':\n",
    "            error_pairs[venue] = location_dict[venue].raw['type']\n",
    "    except:\n",
    "        error_pairs[venue] = 'None'\n",
    "\n",
    "# Searching on the net we found the correct names for stadiums that were erroneously mapped.\n",
    "# We will do a new geolocation search for the new names later\n",
    "\n",
    "\n",
    "rename_dict = {'Miller Park':'American Family Field',\n",
    "'O.co Coliseum': 'RingCentral Coliseum',\n",
    "'U.S. Cellular Field' : 'Guaranteed Rate Field',\n",
    "'AT&T Park' : 'Oracle Park',\n",
    "'Oakland Coliseum' : 'RingCentral Coliseum',\n",
    "'Fort Bragg Field' : 'Fort Bragg Field baseball',\n",
    "'Williamsport Little League Classic' : 'Williamsport, Pennsylvania',\n",
    "'Angel Stadium': 'Angel Stadium of Anaheim',\n",
    "'BB&T Ballpark': 'Williamsport, Pennsylvania'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('American Family Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# We will take the new dictionary, which is rename dict, and find every geolocatoin that correspond to the new names\n",
    "# after that we append these new names to the already existing location_dict\n",
    "\n",
    "for k,v in rename_dict.items():\n",
    "    location_dict[v] = geocoder(v)\n",
    "    \n",
    "# After that we remove the previously found, but erroneous names\n",
    "\n",
    "for key in [k for k,v in rename_dict.items()]:\n",
    "    if key in location_dict:\n",
    "        del location_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create subdf, which is simple table that has unique combinations of dates and venues\n",
    "subdf = df[['date','venue_name']].drop_duplicates()\n",
    "\n",
    "# We map the new venue name to the dataframe we created earlier\n",
    "to_replace = [k for k,v in rename_dict.items()]\n",
    "\n",
    "new_venue = []\n",
    "\n",
    "for i in subdf['venue_name']:\n",
    "    if i in to_replace:\n",
    "        new_venue.append(rename_dict[i])\n",
    "    else:\n",
    "        new_venue.append(i)\n",
    "\n",
    "subdf['new_venue'] = new_venue\n",
    "\n",
    "# Now we add locational data and longitudes and latitudes\n",
    "subdf['location'] = [location_dict[i] for i in subdf['new_venue']]\n",
    "subdf['latitude'] = [i.latitude for i in subdf['location']]\n",
    "subdf['longitude'] = [i.longitude for i in subdf['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing resulting df to disk\n",
    "\n",
    "subdf.to_csv('date_n_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weather data with API \n",
    "\n",
    "I will use the Visualcrossing API which is free until 1000 requests/day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "# We will filter out everything that did not happen in the past two years. For this we first need to convert \n",
    "# the date variable to have date datatype using the datetime package\n",
    "\n",
    "subdf['date'] = subdf.apply(lambda x : datetime.strptime(x['date'], \"%Y-%m-%d\").date(),axis= 1)\n",
    "subdf = subdf[subdf['date'] > datetime.strptime('2017-04-01','%Y-%m-%d').date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is commented out so that there is no chance I rerun it\n",
    "# This cell is to read in the API key stored in a separate txt file on my PC\n",
    "\n",
    "#f = open(r\"C:\\Users\\T450s\\Desktop\\api\\weather/visualcrossing_weather.txt\", \"r\")\n",
    "#api_key = f.readline()\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create API link\n",
    "\n",
    "# We are using VisualCrossing Weather Data Services in order to query historic weather data.\n",
    "# We pay 0.0001$/record, so we will be extra careful when requesting data. \n",
    "# Documentation for the API is available from here: \n",
    "    # https://www.visualcrossing.com/resources/documentation/weather-data/getting-started-with-weather-data-services/\n",
    "\n",
    "# We will be using daily data and request the info to come to us as a json\n",
    "\n",
    "def keygen(latitude,longitude,hours,start_date,end_date,json_or_csv,api_key):\n",
    "    \n",
    "    BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/'\n",
    "    hist_forecast = 'history/'\n",
    "    \n",
    "    \n",
    "    locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "    aggregateHours = 'aggregateHours=' + str(hours)\n",
    "    startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "    endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "    unitGroup = 'unitGroup='+'uk'\n",
    "    contentType = 'contentType='+json_or_csv\n",
    "    dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "    dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "    key = 'key=' + str(api_key)\n",
    "    \n",
    "    \n",
    "    res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "    request_link = BaseURL+hist_forecast + res\n",
    "    return(request_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will drop a few variables because they are events happening with very low frequnency \n",
    "# (maybe some temporary fields or special competitions) - it is not worth including these in the analysis\n",
    "\n",
    "to_drop = ['Estadio de Beisbol Monterrey', 'Fort Bragg Field baseball', 'Hiram Bithorn Stadium', 'Williamsport, Pennsylvania']\n",
    "subdf = subdf[~subdf['new_venue'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to create an API dataframe the purpose of which is to provide a list of links that can be used for the API\n",
    "# We need a start date, end date, latitude, longitud information to create API links\n",
    "\n",
    "\n",
    "# create a column with the start date\n",
    "start_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'min'}).rename(columns = {'date':'start_date'}).reset_index()\n",
    "\n",
    "# create a column with end date\n",
    "end_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'max'}).rename(columns = {'date':'end_date'}).reset_index()\n",
    "\n",
    "# merge dfs \n",
    "api_df = pd.merge(start_df,end_df)\n",
    "\n",
    "# create links with custom function specified above\n",
    "api_df['links'] = api_df.apply(lambda x: keygen(x['latitude'],x['longitude'],24,x['start_date'],x['end_date'],'json',api_key) ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write api_df to disk as we will use that for joining weather df + subdf \n",
    "\n",
    "api_df.to_csv('api_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.074552350000005,-118.24004805779221\n",
      "33.8002599,-117.88174262759796\n",
      "32.70718815,-117.15687745290563\n",
      "33.44548555,-112.06669283293144\n",
      "39.756031400000005,-104.99292855531492\n",
      "32.7513904,-97.08347649984135\n",
      "29.75723205,-95.35523391491142\n",
      "39.05144525,-94.48047131020273\n",
      "44.9817039,-93.2778457595517\n",
      "38.62255379999999,-90.19392193458769\n",
      "43.0280619,-87.97209586840819\n",
      "41.9481225,-87.6563513145702\n",
      "41.8300162,-87.63256264837347\n",
      "39.09724845,-84.50662325533993\n",
      "33.89070855,-84.46853422885837\n",
      "42.33915895,-83.04962481782741\n",
      "27.7680559,-82.65327550461797\n",
      "41.4960144,-81.68420022215649\n",
      "25.7782474,-80.21980500744203\n",
      "40.446925799999995,-80.00560626612204\n",
      "43.6416641,-79.38919882366382\n",
      "38.87274095,-77.00838588569519\n",
      "39.28398230000001,-76.62249149865416\n",
      "39.90588575,-75.16541101747245\n",
      "40.82958275,-73.92652118491901\n",
      "40.75727785,-73.84587884942417\n",
      "42.346462100000004,-71.0971002033302\n"
     ]
    }
   ],
   "source": [
    "# create empty list to append df to cache dfs resulting from interation\n",
    "weather_df = []\n",
    "\n",
    "for i, c in api_df.iterrows():\n",
    "    \n",
    "    # We first get a json from API and save it to myjson variable\n",
    "    myjson = re.get(api_df.loc[i,'links'])\n",
    "    \n",
    "    # weather_json will hold the parsed json that is now a dictionary \n",
    "    weather_json = json.loads(myjson.text)\n",
    "    \n",
    "    # get coordinates as json's structure needs such a key\n",
    "    my_coords =  [str(k) for k in [weather_json['locations'].keys()][0]][0]\n",
    "    \n",
    "    # printing to see status only\n",
    "    print(str(api_df.loc[i,'latitude']) + ',' + str(api_df.loc[i,'longitude']), 'is ready')\n",
    "    \n",
    "    # create temporary dataframe out of json\n",
    "    temp_weather_df = pd.DataFrame(weather_json['locations'][my_coords]['values'])\n",
    "    \n",
    "    # add location var so that it can be joined with sub df \n",
    "    temp_weather_df['location'] = my_coords\n",
    "\n",
    "    # append to list created in the beginning to concatenate them \n",
    "    weather_df.append(temp_weather_df)\n",
    "\n",
    "# concatenate list of dfs into one final df \n",
    "weather_df = pd.concat(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weather data to disk so that we don't have to rerun API requests - this would cost money.\n",
    "\n",
    "weather_df.to_csv('weather_df_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Subdf and Weather df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wdir</th>\n",
       "      <th>temp</th>\n",
       "      <th>maxt</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wspd</th>\n",
       "      <th>datetimeStr</th>\n",
       "      <th>solarenergy</th>\n",
       "      <th>heatindex</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>mint</th>\n",
       "      <th>datetime</th>\n",
       "      <th>precip</th>\n",
       "      <th>solarradiation</th>\n",
       "      <th>weathertype</th>\n",
       "      <th>snowdepth</th>\n",
       "      <th>sealevelpressure</th>\n",
       "      <th>snow</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipcover</th>\n",
       "      <th>wgust</th>\n",
       "      <th>conditions</th>\n",
       "      <th>windchill</th>\n",
       "      <th>info</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198.54</td>\n",
       "      <td>12.9</td>\n",
       "      <td>18.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2017-04-10T00:00:00-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.3</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1491782400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>70.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7786119,-122.3902674542564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186.33</td>\n",
       "      <td>15.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2017-04-11T00:00:00-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1491868800000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mist, Squalls, Light Rain</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>66.47</td>\n",
       "      <td>4.17</td>\n",
       "      <td>32.7</td>\n",
       "      <td>Rain, Partially cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7786119,-122.3902674542564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216.75</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2017-04-12T00:00:00-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>1491955200000</td>\n",
       "      <td>5.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mist, Light Rain</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>75.32</td>\n",
       "      <td>45.83</td>\n",
       "      <td>29.8</td>\n",
       "      <td>Rain, Partially cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7786119,-122.3902674542564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254.79</td>\n",
       "      <td>13.5</td>\n",
       "      <td>15.4</td>\n",
       "      <td>9.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2017-04-13T00:00:00-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1492041600000</td>\n",
       "      <td>2.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rain, Light Rain</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>69.33</td>\n",
       "      <td>16.67</td>\n",
       "      <td>30.6</td>\n",
       "      <td>Rain, Partially cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7786119,-122.3902674542564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309.13</td>\n",
       "      <td>12.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>22.8</td>\n",
       "      <td>2017-04-14T00:00:00-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1492128000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>58.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.1</td>\n",
       "      <td>Clear</td>\n",
       "      <td>6.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7786119,-122.3902674542564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wdir  temp  maxt  visibility  wspd                datetimeStr  \\\n",
       "0  198.54  12.9  18.5         9.9  16.9  2017-04-10T00:00:00-07:00   \n",
       "1  186.33  15.5  18.6         9.8  20.1  2017-04-11T00:00:00-07:00   \n",
       "2  216.75  15.4  18.3         8.0  21.2  2017-04-12T00:00:00-07:00   \n",
       "3  254.79  13.5  15.4         9.7  21.5  2017-04-13T00:00:00-07:00   \n",
       "4  309.13  12.5  16.8         9.9  22.8  2017-04-14T00:00:00-07:00   \n",
       "\n",
       "   solarenergy  heatindex  cloudcover  mint       datetime  precip  \\\n",
       "0          NaN        NaN        43.3   8.3  1491782400000    0.00   \n",
       "1          NaN        NaN        73.8  12.8  1491868800000    0.12   \n",
       "2          NaN        NaN        70.0  13.1  1491955200000    5.14   \n",
       "3          NaN        NaN        55.5  10.9  1492041600000    2.11   \n",
       "4          NaN        NaN        19.2   8.3  1492128000000    0.00   \n",
       "\n",
       "   solarradiation                weathertype  snowdepth  sealevelpressure  \\\n",
       "0             NaN                        NaN        0.0            1021.8   \n",
       "1             NaN  Mist, Squalls, Light Rain        0.0            1015.9   \n",
       "2             NaN           Mist, Light Rain        0.0            1014.5   \n",
       "3             NaN           Rain, Light Rain        0.0            1016.9   \n",
       "4             NaN                        NaN        0.0            1022.5   \n",
       "\n",
       "   snow   dew  humidity  precipcover  wgust              conditions  \\\n",
       "0   0.0   7.6     70.74         0.00   23.0        Partially cloudy   \n",
       "1   0.0   9.1     66.47         4.17   32.7  Rain, Partially cloudy   \n",
       "2   0.0  10.8     75.32        45.83   29.8  Rain, Partially cloudy   \n",
       "3   0.0   7.8     69.33        16.67   30.6  Rain, Partially cloudy   \n",
       "4   0.0   4.4     58.44         0.00   31.1                   Clear   \n",
       "\n",
       "   windchill  info                       location  \n",
       "0        7.3   NaN  37.7786119,-122.3902674542564  \n",
       "1        NaN   NaN  37.7786119,-122.3902674542564  \n",
       "2        NaN   NaN  37.7786119,-122.3902674542564  \n",
       "3        NaN   NaN  37.7786119,-122.3902674542564  \n",
       "4        6.3   NaN  37.7786119,-122.3902674542564  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_df = pd.read_csv('api_df.csv', index_col = 'Unnamed: 0')\n",
    "weather_df = pd.read_csv('weather_df_final.csv',index_col = 'Unnamed: 0')\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO merge api_df, weather_df, subdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "\n",
    "#### Below codes are not used \n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wrigley Field': 'Chicago',\n",
       " 'Safeco Field': 'Seattle',\n",
       " 'Marlins Park': 'Miami',\n",
       " 'Tropicana Field': 'St. Petersburg',\n",
       " 'Citizens Bank Park': 'Philadelphia',\n",
       " 'Kauffman Stadium': 'Kansas City',\n",
       " 'Minute Maid Park': 'Houston',\n",
       " 'Comerica Park': 'Detroit',\n",
       " 'Nationals Park': 'Washington',\n",
       " 'Great American Ball Park': 'Cincinnati',\n",
       " 'Dodger Stadium': 'Los Angeles',\n",
       " 'Chase Field': 'Phoenix',\n",
       " 'Yankee Stadium': 'New York',\n",
       " 'Petco Park': 'San Diego',\n",
       " 'Coors Field': 'Denver',\n",
       " 'Progressive Field': 'Cleveland',\n",
       " 'Globe Life Park in Arlington': 'Arlington',\n",
       " 'Angel Stadium of Anaheim': 'Anaheim',\n",
       " 'Turner Field': 'Atlanta',\n",
       " 'Oriole Park at Camden Yards': 'Baltimore',\n",
       " 'PNC Park': 'Pittsburgh',\n",
       " 'Target Field': 'Minneapolis',\n",
       " 'Busch Stadium': 'City of Saint Louis',\n",
       " 'Citi Field': 'New York',\n",
       " 'Rogers Centre': 'Old Toronto',\n",
       " 'Fenway Park': 'Boston',\n",
       " 'Guaranteed Rate Field': 'Chicago',\n",
       " 'SunTrust Park': 'Atlanta',\n",
       " 'Hiram Bithorn Stadium': 'San Juan',\n",
       " 'Estadio de Beisbol Monterrey': 'Monterrey',\n",
       " 'American Family Field': 'Milwaukee',\n",
       " 'RingCentral Coliseum': 'Oakland',\n",
       " 'Oracle Park': 'San Francisco',\n",
       " 'Fort Bragg Field baseball': 'Fort Bragg',\n",
       " 'Williamsport, Pennsylvania': 'Williamsport'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOT USED \n",
    "\n",
    "## this is a code to get back city names from geolocations \n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "# geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# interim_dict = {}\n",
    "# city_dict = {}\n",
    "\n",
    "# for k,v in location_dict.items(): \n",
    "#     interim_dict[k]=((location_dict[k].latitude,location_dict[k].longitude))\n",
    "    \n",
    "# for k,v in interim_dict.items(): \n",
    "#     city_dict[k] = geolocator.reverse(interim_dict[k]).raw['address'].get('city','')\n",
    "    \n",
    "# city_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is not used \n",
    "\n",
    "# # doing one sample request from the API \n",
    "\n",
    "# BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/'\n",
    "# hist_forecast = 'history/'\n",
    "\n",
    "# latitude = 41.9481225\n",
    "# longitude = -87.6563513145702\n",
    "# hours = 24\n",
    "# start_date = '2019-06-13'\n",
    "# end_date = '2019-06-13'\n",
    "# json_or_csv = 'json'\n",
    "\n",
    "\n",
    "# # specifying API parameters \n",
    "# locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "# aggregateHours = 'aggregateHours=' + str(hours)\n",
    "# startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "# endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "# unitGroup = 'unitGroup='+'uk'\n",
    "# contentType = 'contentType='+json_or_csv\n",
    "# dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "# dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "# key = 'key=' + str(api_key)\n",
    "\n",
    "# # create request link \n",
    "\n",
    "\n",
    "# res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "\n",
    "# request_link = BaseURL+hist_forecast + res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
