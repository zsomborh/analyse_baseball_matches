{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to get weather data\n",
    "\n",
    "The purpose of this notebook is to create a dataframe which has weather data for given venue in given date. This is done in three major parts:\n",
    "\n",
    "1. Create geolocations and get longitude and latitude data for given baseball venue\n",
    "2. Use VisualCrossing API to get weather data for all matches between the 2017 and 2018 seasion in given location in given day. \n",
    "3. Merge and clean these dataframes so that they can be used for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import requests as re \n",
    "\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Getting geolocations based on venue\n",
    "\n",
    "We load up a preprocessed dataframe, and get all locational data with the use of geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "\n",
    "df = pd.read_csv('Baseball_Merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Wrigley Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Great American Ball Park',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# We initialise a geocoder object using the combination of RateLimiter and Nominatim (both are from the geopy package)\n",
    "# these will be used to get geolocations based on data that we have, which is the name of the baseball pitch\n",
    "\n",
    "geocoder = RateLimiter(Nominatim(user_agent='ba').geocode, min_delay_seconds=1)\n",
    "\n",
    "# We will get the unique venues so that we can apply geocoder to those\n",
    "venues = df['venue_name'].unique()\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for i in venues: \n",
    "    location_dict[i] = geocoder(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoder is very powerful but it is not working 100% given that we only feed in the name of a baseball pitch\n",
    "    # some of which might not even exist anymore\n",
    "\n",
    "# We will identify potentially erroneous locations by looking at the type of the observation - if this is not a stadium\n",
    "# we do a manual inspection \n",
    "    \n",
    "error_pairs = {}\n",
    "for venue in venues:\n",
    "    try:\n",
    "        if location_dict[venue].raw['type'] != 'stadium':\n",
    "            error_pairs[venue] = location_dict[venue].raw['type']\n",
    "    except:\n",
    "        error_pairs[venue] = 'None'\n",
    "\n",
    "# Searching on the net we found the correct names for stadiums that were erroneously mapped.\n",
    "# We will do a new geolocation search for the new names later\n",
    "\n",
    "\n",
    "rename_dict = {'Miller Park':'American Family Field',\n",
    "'O.co Coliseum': 'RingCentral Coliseum',\n",
    "'U.S. Cellular Field' : 'Guaranteed Rate Field',\n",
    "'AT&T Park' : 'Oracle Park',\n",
    "'Oakland Coliseum' : 'RingCentral Coliseum',\n",
    "'Fort Bragg Field' : 'Fort Bragg Field baseball',\n",
    "'Williamsport Little League Classic' : 'Williamsport, Pennsylvania',\n",
    "'Angel Stadium': 'Angel Stadium of Anaheim',\n",
    "'BB&T Ballpark': 'Williamsport, Pennsylvania'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('American Family Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# We will take the new dictionary, which is rename dict, and find every geolocatoin that correspond to the new names\n",
    "# after that we append these new names to the already existing location_dict\n",
    "\n",
    "for k,v in rename_dict.items():\n",
    "    location_dict[v] = geocoder(v)\n",
    "    \n",
    "# After that we remove the previously found, but erroneous names\n",
    "\n",
    "for key in [k for k,v in rename_dict.items()]:\n",
    "    if key in location_dict:\n",
    "        del location_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create subdf, which is simple table that has unique combinations of dates and venues\n",
    "subdf = df[['date','venue_name']].drop_duplicates()\n",
    "\n",
    "# We map the new venue name to the dataframe we created earlier\n",
    "to_replace = [k for k,v in rename_dict.items()]\n",
    "\n",
    "new_venue = []\n",
    "\n",
    "for i in subdf['venue_name']:\n",
    "    if i in to_replace:\n",
    "        new_venue.append(rename_dict[i])\n",
    "    else:\n",
    "        new_venue.append(i)\n",
    "\n",
    "subdf['new_venue'] = new_venue\n",
    "\n",
    "# Now we add locational data and longitudes and latitudes\n",
    "subdf['location'] = [location_dict[i] for i in subdf['new_venue']]\n",
    "subdf['latitude'] = [i.latitude for i in subdf['location']]\n",
    "subdf['longitude'] = [i.longitude for i in subdf['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing resulting df to disk\n",
    "\n",
    "subdf.to_csv('date_n_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get weather data with API \n",
    "\n",
    "We will use the Visualcrossing API to get weather data for given location on given day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "# We will filter out everything that did not happen in the past two years. For this we first need to convert \n",
    "# the date variable to have date datatype using the datetime package\n",
    "\n",
    "subdf['date'] = subdf.apply(lambda x : datetime.strptime(x['date'], \"%Y-%m-%d\").date(),axis= 1)\n",
    "subdf = subdf[subdf['date'] > datetime.strptime('2017-04-01','%Y-%m-%d').date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is commented out so that there is no chance I rerun it\n",
    "# This cell is to read in the API key stored in a separate txt file on my PC\n",
    "\n",
    "#f = open(r\"C:\\Users\\T450s\\Desktop\\api\\weather/visualcrossing_weather.txt\", \"r\")\n",
    "#api_key = f.readline()\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create API link\n",
    "\n",
    "# We are using VisualCrossing Weather Data Services in order to query historic weather data.\n",
    "# We pay 0.0001$/record, so we will be extra careful when requesting data. \n",
    "# Documentation for the API is available from here: \n",
    "    # https://www.visualcrossing.com/resources/documentation/weather-data/getting-started-with-weather-data-services/\n",
    "\n",
    "# We will be using daily data and request the info to come to us as a json\n",
    "\n",
    "def keygen(latitude,longitude,hours,start_date,end_date,json_or_csv,api_key):\n",
    "    \n",
    "    BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/'\n",
    "    hist_forecast = 'history/'\n",
    "    \n",
    "    \n",
    "    locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "    aggregateHours = 'aggregateHours=' + str(hours)\n",
    "    startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "    endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "    unitGroup = 'unitGroup='+'uk'\n",
    "    contentType = 'contentType='+json_or_csv\n",
    "    dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "    dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "    key = 'key=' + str(api_key)\n",
    "    \n",
    "    \n",
    "    res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "    request_link = BaseURL+hist_forecast + res\n",
    "    return(request_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will drop a few variables because they are events happening with very low frequnency \n",
    "# (maybe some temporary fields or special competitions) - it is not worth including these in the analysis\n",
    "\n",
    "to_drop = ['Estadio de Beisbol Monterrey', 'Fort Bragg Field baseball', 'Hiram Bithorn Stadium', 'Williamsport, Pennsylvania']\n",
    "subdf = subdf[~subdf['new_venue'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to create an API dataframe the purpose of which is to provide a list of links that can be used for the API\n",
    "# We need a start date, end date, latitude, longitud information to create API links\n",
    "\n",
    "\n",
    "# create a column with the start date\n",
    "start_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'min'}).rename(columns = {'date':'start_date'}).reset_index()\n",
    "\n",
    "# create a column with end date\n",
    "end_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'max'}).rename(columns = {'date':'end_date'}).reset_index()\n",
    "\n",
    "# merge dfs \n",
    "api_df = pd.merge(start_df,end_df)\n",
    "\n",
    "# create links with custom function specified above\n",
    "api_df['links'] = api_df.apply(lambda x: keygen(x['latitude'],x['longitude'],24,x['start_date'],x['end_date'],'json',api_key) ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.074552350000005,-118.24004805779221\n",
      "33.8002599,-117.88174262759796\n",
      "32.70718815,-117.15687745290563\n",
      "33.44548555,-112.06669283293144\n",
      "39.756031400000005,-104.99292855531492\n",
      "32.7513904,-97.08347649984135\n",
      "29.75723205,-95.35523391491142\n",
      "39.05144525,-94.48047131020273\n",
      "44.9817039,-93.2778457595517\n",
      "38.62255379999999,-90.19392193458769\n",
      "43.0280619,-87.97209586840819\n",
      "41.9481225,-87.6563513145702\n",
      "41.8300162,-87.63256264837347\n",
      "39.09724845,-84.50662325533993\n",
      "33.89070855,-84.46853422885837\n",
      "42.33915895,-83.04962481782741\n",
      "27.7680559,-82.65327550461797\n",
      "41.4960144,-81.68420022215649\n",
      "25.7782474,-80.21980500744203\n",
      "40.446925799999995,-80.00560626612204\n",
      "43.6416641,-79.38919882366382\n",
      "38.87274095,-77.00838588569519\n",
      "39.28398230000001,-76.62249149865416\n",
      "39.90588575,-75.16541101747245\n",
      "40.82958275,-73.92652118491901\n",
      "40.75727785,-73.84587884942417\n",
      "42.346462100000004,-71.0971002033302\n"
     ]
    }
   ],
   "source": [
    "# create empty list to append df to cache dfs resulting from interation\n",
    "weather_df = []\n",
    "\n",
    "for i, c in api_df.iterrows():\n",
    "    \n",
    "    # We first get a json from API and save it to myjson variable\n",
    "    myjson = re.get(api_df.loc[i,'links'])\n",
    "    \n",
    "    # weather_json will hold the parsed json that is now a dictionary \n",
    "    weather_json = json.loads(myjson.text)\n",
    "    \n",
    "    # get coordinates as json's structure needs such a key\n",
    "    my_coords =  [str(k) for k in [weather_json['locations'].keys()][0]][0]\n",
    "    \n",
    "    # printing to see status only\n",
    "    print(str(api_df.loc[i,'latitude']) + ',' + str(api_df.loc[i,'longitude']), 'is ready')\n",
    "    \n",
    "    # create temporary dataframe out of json\n",
    "    temp_weather_df = pd.DataFrame(weather_json['locations'][my_coords]['values'])\n",
    "    \n",
    "    # add location var so that it can be joined with sub df \n",
    "    temp_weather_df['location'] = my_coords\n",
    "\n",
    "    # append to list created in the beginning to concatenate them \n",
    "    weather_df.append(temp_weather_df)\n",
    "\n",
    "# concatenate list of dfs into one final df \n",
    "weather_df = pd.concat(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weather data to disk so that we don't have to rerun API requests - this would cost money.\n",
    "\n",
    "weather_df.to_csv('weather_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Merge resulting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up dataframes from disk\n",
    "\n",
    "weather_df = pd.read_csv('weather_df.csv',index_col = 'Unnamed: 0')\n",
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some minor formatting to be used for joining \n",
    "\n",
    "subdf['latlong'] = subdf['latitude'].astype('str') +',' + subdf['longitude'].astype('str')\n",
    "weather_df['date'] = [datetime.fromtimestamp(date/1000).strftime('%Y-%m-%d') for date in weather_df['datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the datasets so now we will have date-location-weather data in the same dataframe\n",
    "\n",
    "merged_df = pd.merge(left = subdf, \n",
    "         right = weather_df,\n",
    "         left_on = ['date', 'latlong'],\n",
    "         right_on = ['date','location' ], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this dataframe so that it only has columns that are useful\n",
    "\n",
    "merged_df = merged_df[['date', 'venue_name', 'latitude','longitude',\n",
    "                       'wdir', 'temp', 'maxt', 'visibility', 'wspd', \n",
    "                       'cloudcover', 'mint', 'precip', 'snowdepth', \n",
    "                      'dew', 'humidity','precipcover']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>wdir</th>\n",
       "      <th>temp</th>\n",
       "      <th>maxt</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wspd</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>mint</th>\n",
       "      <th>precip</th>\n",
       "      <th>snowdepth</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipcover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Busch Stadium</td>\n",
       "      <td>38.622554</td>\n",
       "      <td>-90.193922</td>\n",
       "      <td>116.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>19.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>12.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.63</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Tropicana Field</td>\n",
       "      <td>27.768056</td>\n",
       "      <td>-82.653276</td>\n",
       "      <td>135.58</td>\n",
       "      <td>24.3</td>\n",
       "      <td>29.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>73.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>Chase Field</td>\n",
       "      <td>33.445486</td>\n",
       "      <td>-112.066693</td>\n",
       "      <td>161.79</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>12.7</td>\n",
       "      <td>22.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>29.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Oakland Coliseum</td>\n",
       "      <td>37.751675</td>\n",
       "      <td>-122.199371</td>\n",
       "      <td>200.42</td>\n",
       "      <td>14.9</td>\n",
       "      <td>20.6</td>\n",
       "      <td>9.5</td>\n",
       "      <td>17.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>63.81</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Citi Field</td>\n",
       "      <td>40.757278</td>\n",
       "      <td>-73.845879</td>\n",
       "      <td>194.04</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        venue_name   latitude   longitude    wdir  temp  maxt  \\\n",
       "0  2017-04-02     Busch Stadium  38.622554  -90.193922  116.71  13.3  19.4   \n",
       "1  2017-04-02   Tropicana Field  27.768056  -82.653276  135.58  24.3  29.4   \n",
       "2  2017-04-02       Chase Field  33.445486 -112.066693  161.79  20.0  27.1   \n",
       "3  2017-04-03  Oakland Coliseum  37.751675 -122.199371  200.42  14.9  20.6   \n",
       "4  2017-04-03        Citi Field  40.757278  -73.845879  194.04  12.9  16.6   \n",
       "\n",
       "   visibility  wspd  cloudcover  mint  precip  snowdepth   dew  humidity  \\\n",
       "0         9.9  12.5        11.0   7.3    0.25        0.0   7.0     66.63   \n",
       "1         9.9  11.1         5.5  21.0    0.00        0.0  18.9     73.06   \n",
       "2         9.9  12.7        22.6  12.1    0.00        0.0   0.6     29.90   \n",
       "3         9.5  17.1        17.0   8.9    0.00        0.0   7.5     63.81   \n",
       "4         9.9  17.0        65.7   8.8    0.00        0.0   0.2     42.47   \n",
       "\n",
       "   precipcover  \n",
       "0         4.17  \n",
       "1         0.00  \n",
       "2         0.00  \n",
       "3         0.00  \n",
       "4         0.00  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('location_weather.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
