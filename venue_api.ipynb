{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "#from geopy import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Baseball_Merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Wrigley Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Wrigley+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('Great American Ball Park',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Great+American+Ball+Park&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# RateLimiter must be used for Nominatim\n",
    "geocoder = RateLimiter(Nominatim(user_agent='ba').geocode, min_delay_seconds=1)\n",
    "venues = df['venue_name'].unique()\n",
    "\n",
    "location_dict = {}\n",
    "\n",
    "for i in venues: \n",
    "    location_dict[i] = geocoder(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are erroneous pairings\n",
    "error_pairs = {}\n",
    "for venue in venues:\n",
    "    try:\n",
    "        if location_dict[venue].raw['type'] != 'stadium':\n",
    "            error_pairs[venue] = location_dict[venue].raw['type']\n",
    "    except:\n",
    "        error_pairs[venue] = 'None'\n",
    "\n",
    "# When looking at online, found good string for geopy\n",
    "rename_dict = {'Miller Park':'American Family Field',\n",
    "'O.co Coliseum': 'RingCentral Coliseum',\n",
    "'U.S. Cellular Field' : 'Guaranteed Rate Field',\n",
    "'AT&T Park' : 'Oracle Park',\n",
    "'Oakland Coliseum' : 'RingCentral Coliseum',\n",
    "'Fort Bragg Field' : 'Fort Bragg Field baseball',\n",
    "'Williamsport Little League Classic' : 'Williamsport, Pennsylvania',\n",
    "'Angel Stadium': 'Angel Stadium of Anaheim',\n",
    "'BB&T Ballpark': 'Williamsport, Pennsylvania'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('American Family Field',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 423, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 331, in _raise_timeout\n",
      "    self, url, \"Read timed out. (read timeout=%s)\" % timeout_value\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 760, in urlopen\n",
      "    **response_kw\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 387, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 546, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\extra\\rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\geocoders\\base.py\", line 360, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 377, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"C:\\Users\\T450s\\Anaconda3\\lib\\site-packages\\geopy\\adapters.py\", line 399, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=American+Family+Field&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n"
     ]
    }
   ],
   "source": [
    "# Get locations to each venue\n",
    "for k,v in rename_dict.items():\n",
    "    location_dict[v] = geocoder(v)\n",
    "\n",
    "for key in [k for k,v in rename_dict.items()]:\n",
    "    if key in location_dict:\n",
    "        del location_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need all date-venue_name pairs for the API \n",
    "subdf = df[['date','venue_name']].drop_duplicates()\n",
    "\n",
    "# we require a new col where new venue names are used\n",
    "to_replace = [k for k,v in rename_dict.items()]\n",
    "\n",
    "new_venue = []\n",
    "\n",
    "for i in subdf['venue_name']:\n",
    "    if i in to_replace:\n",
    "        new_venue.append(rename_dict[i])\n",
    "    else:\n",
    "        new_venue.append(i)\n",
    "\n",
    "subdf['new_venue'] = new_venue\n",
    "\n",
    "# map location based on new venue\n",
    "subdf['location'] = [location_dict[i] for i in subdf['new_venue']]\n",
    "subdf['latitude'] = [i.latitude for i in subdf['location']]\n",
    "subdf['longitude'] = [i.longitude for i in subdf['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>new_venue</th>\n",
       "      <th>location</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-05</td>\n",
       "      <td>Wrigley Field</td>\n",
       "      <td>Wrigley Field</td>\n",
       "      <td>(Wrigley Field, 1060, West Addison Street, Wri...</td>\n",
       "      <td>41.948122</td>\n",
       "      <td>-87.656351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>Safeco Field</td>\n",
       "      <td>Safeco Field</td>\n",
       "      <td>(T-Mobile Park, 3rd Avenue South, Internationa...</td>\n",
       "      <td>47.591414</td>\n",
       "      <td>-122.332023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>Marlins Park</td>\n",
       "      <td>Marlins Park</td>\n",
       "      <td>(Marlins Park, Northwest 15th Avenue, Miami, M...</td>\n",
       "      <td>25.778247</td>\n",
       "      <td>-80.219805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>Tropicana Field</td>\n",
       "      <td>Tropicana Field</td>\n",
       "      <td>(Tropicana Field, Pinellas Trail, Methodist To...</td>\n",
       "      <td>27.768056</td>\n",
       "      <td>-82.653276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>Citizens Bank Park</td>\n",
       "      <td>Citizens Bank Park</td>\n",
       "      <td>(Citizens Bank Park, Pattison Avenue, South Ph...</td>\n",
       "      <td>39.905886</td>\n",
       "      <td>-75.165411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date          venue_name           new_venue  \\\n",
       "0     2015-04-05       Wrigley Field       Wrigley Field   \n",
       "314   2015-04-06        Safeco Field        Safeco Field   \n",
       "567   2015-04-06        Marlins Park        Marlins Park   \n",
       "810   2015-04-06     Tropicana Field     Tropicana Field   \n",
       "1105  2015-04-06  Citizens Bank Park  Citizens Bank Park   \n",
       "\n",
       "                                               location   latitude   longitude  \n",
       "0     (Wrigley Field, 1060, West Addison Street, Wri...  41.948122  -87.656351  \n",
       "314   (T-Mobile Park, 3rd Avenue South, Internationa...  47.591414 -122.332023  \n",
       "567   (Marlins Park, Northwest 15th Avenue, Miami, M...  25.778247  -80.219805  \n",
       "810   (Tropicana Field, Pinellas Trail, Methodist To...  27.768056  -82.653276  \n",
       "1105  (Citizens Bank Park, Pattison Avenue, South Ph...  39.905886  -75.165411  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf.to_csv('date_n_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weather data with API \n",
    "\n",
    "I will use the Visualcrossing API which is free until 1000 requests/day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re \n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf= pd.read_csv('date_n_location.csv', index_col = 'Unnamed: 0')\n",
    "subdf['date'] = subdf.apply(lambda x : datetime.strptime(x['date'], \"%Y-%m-%d\").date(),axis= 1)\n",
    "subdf = subdf[subdf['date'] > datetime.strptime('2017-04-01','%Y-%m-%d').date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keygen(latitude,longitude,hours,start_date,end_date,json_or_csv,api_key):\n",
    "    locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "    aggregateHours = 'aggregateHours=' + str(hours)\n",
    "    startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "    endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "    unitGroup = 'unitGroup='+'uk'\n",
    "    contentType = 'contentType='+json_or_csv\n",
    "    dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "    dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "    key = 'key=' + str(api_key)\n",
    "    \n",
    "    \n",
    "    res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "    request_link = BaseURL+hist_forecast + res\n",
    "    return(request_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Estadio de Beisbol Monterrey', 'Fort Bragg Field baseball', 'Hiram Bithorn Stadium', 'Williamsport, Pennsylvania']\n",
    "subdf = subdf[~subdf['new_venue'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'min'}).rename(columns = {'date':'start_date'}).reset_index()\n",
    "end_df = subdf.groupby(['longitude', 'latitude']).agg({'date':'max'}).rename(columns = {'date':'end_date'}).reset_index()\n",
    "api_df = pd.merge(start_df,end_df)\n",
    "links = api_df.apply(lambda x: keygen(x['latitude'],x['longitude'],hours,x['start_date'],x['end_date'],json_or_csv,api_key) ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.390267</td>\n",
       "      <td>37.778612</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.332023</td>\n",
       "      <td>47.591414</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.199371</td>\n",
       "      <td>37.751675</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-118.240048</td>\n",
       "      <td>34.074552</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-117.881743</td>\n",
       "      <td>33.800260</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-117.156877</td>\n",
       "      <td>32.707188</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-112.066693</td>\n",
       "      <td>33.445486</td>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>2018-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-104.992929</td>\n",
       "      <td>39.756031</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-97.083476</td>\n",
       "      <td>32.751390</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-95.355234</td>\n",
       "      <td>29.757232</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-94.480471</td>\n",
       "      <td>39.051445</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-93.277846</td>\n",
       "      <td>44.981704</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-90.193922</td>\n",
       "      <td>38.622554</td>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>2018-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-87.972096</td>\n",
       "      <td>43.028062</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-87.656351</td>\n",
       "      <td>41.948122</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2018-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-87.632563</td>\n",
       "      <td>41.830016</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>2018-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-84.506623</td>\n",
       "      <td>39.097248</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-84.468534</td>\n",
       "      <td>33.890709</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-83.049625</td>\n",
       "      <td>42.339159</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-82.653276</td>\n",
       "      <td>27.768056</td>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-81.684200</td>\n",
       "      <td>41.496014</td>\n",
       "      <td>2017-04-11</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-80.219805</td>\n",
       "      <td>25.778247</td>\n",
       "      <td>2017-04-11</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-80.005606</td>\n",
       "      <td>40.446926</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-79.389199</td>\n",
       "      <td>43.641664</td>\n",
       "      <td>2017-04-11</td>\n",
       "      <td>2018-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-77.008386</td>\n",
       "      <td>38.872741</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-76.622491</td>\n",
       "      <td>39.283982</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-75.165411</td>\n",
       "      <td>39.905886</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-73.926521</td>\n",
       "      <td>40.829583</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2018-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-73.845879</td>\n",
       "      <td>40.757278</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-71.097100</td>\n",
       "      <td>42.346462</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     longitude   latitude start_date   end_date\n",
       "0  -122.390267  37.778612 2017-04-10 2018-09-30\n",
       "1  -122.332023  47.591414 2017-04-10 2018-09-30\n",
       "2  -122.199371  37.751675 2017-04-03 2018-09-23\n",
       "3  -118.240048  34.074552 2017-04-03 2018-10-01\n",
       "4  -117.881743  33.800260 2017-04-07 2018-09-30\n",
       "5  -117.156877  32.707188 2017-04-07 2018-09-30\n",
       "6  -112.066693  33.445486 2017-04-02 2018-09-26\n",
       "7  -104.992929  39.756031 2017-04-07 2018-09-30\n",
       "8   -97.083476  32.751390 2017-04-03 2018-09-23\n",
       "9   -95.355234  29.757232 2017-04-03 2018-09-23\n",
       "10  -94.480471  39.051445 2017-04-10 2018-09-30\n",
       "11  -93.277846  44.981704 2017-04-03 2018-09-30\n",
       "12  -90.193922  38.622554 2017-04-02 2018-09-26\n",
       "13  -87.972096  43.028062 2017-04-03 2018-09-30\n",
       "14  -87.656351  41.948122 2017-04-10 2018-10-01\n",
       "15  -87.632563  41.830016 2017-04-04 2018-09-26\n",
       "16  -84.506623  39.097248 2017-04-03 2018-09-30\n",
       "17  -84.468534  33.890709 2017-04-14 2018-09-23\n",
       "18  -83.049625  42.339159 2017-04-07 2018-09-23\n",
       "19  -82.653276  27.768056 2017-04-02 2018-09-30\n",
       "20  -81.684200  41.496014 2017-04-11 2018-09-23\n",
       "21  -80.219805  25.778247 2017-04-11 2018-09-23\n",
       "22  -80.005606  40.446926 2017-04-07 2018-09-23\n",
       "23  -79.389199  43.641664 2017-04-11 2018-09-26\n",
       "24  -77.008386  38.872741 2017-04-03 2018-09-26\n",
       "25  -76.622491  39.283982 2017-04-03 2018-09-30\n",
       "26  -75.165411  39.905886 2017-04-07 2018-09-30\n",
       "27  -73.926521  40.829583 2017-04-10 2018-09-23\n",
       "28  -73.845879  40.757278 2017-04-03 2018-09-30\n",
       "29  -71.097100  42.346462 2017-04-03 2018-09-30"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sign-up with pay as you go option here https://www.visualcrossing.com/weather-data-editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_link = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "myjson = re.get(request_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_json = json.loads(myjson.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'errorCode': 999,\n",
       " 'executionTime': -1,\n",
       " 'sessionId': '',\n",
       " 'message': 'You have exceeded the maximum result row count for your account. Requested 538, Maximum 100. '}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'locations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-1d5e429fd9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweather_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'locations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'41.9481225,-87.6563513145702'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'locations'"
     ]
    }
   ],
   "source": [
    "weather_json['locations']['41.9481225,-87.6563513145702']['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write json parsing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "\n",
    "#### Below codes are not used \n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wrigley Field': 'Chicago',\n",
       " 'Safeco Field': 'Seattle',\n",
       " 'Marlins Park': 'Miami',\n",
       " 'Tropicana Field': 'St. Petersburg',\n",
       " 'Citizens Bank Park': 'Philadelphia',\n",
       " 'Kauffman Stadium': 'Kansas City',\n",
       " 'Minute Maid Park': 'Houston',\n",
       " 'Comerica Park': 'Detroit',\n",
       " 'Nationals Park': 'Washington',\n",
       " 'Great American Ball Park': 'Cincinnati',\n",
       " 'Dodger Stadium': 'Los Angeles',\n",
       " 'Chase Field': 'Phoenix',\n",
       " 'Yankee Stadium': 'New York',\n",
       " 'Petco Park': 'San Diego',\n",
       " 'Coors Field': 'Denver',\n",
       " 'Progressive Field': 'Cleveland',\n",
       " 'Globe Life Park in Arlington': 'Arlington',\n",
       " 'Angel Stadium of Anaheim': 'Anaheim',\n",
       " 'Turner Field': 'Atlanta',\n",
       " 'Oriole Park at Camden Yards': 'Baltimore',\n",
       " 'PNC Park': 'Pittsburgh',\n",
       " 'Target Field': 'Minneapolis',\n",
       " 'Busch Stadium': 'City of Saint Louis',\n",
       " 'Citi Field': 'New York',\n",
       " 'Rogers Centre': 'Old Toronto',\n",
       " 'Fenway Park': 'Boston',\n",
       " 'Guaranteed Rate Field': 'Chicago',\n",
       " 'SunTrust Park': 'Atlanta',\n",
       " 'Hiram Bithorn Stadium': 'San Juan',\n",
       " 'Estadio de Beisbol Monterrey': 'Monterrey',\n",
       " 'American Family Field': 'Milwaukee',\n",
       " 'RingCentral Coliseum': 'Oakland',\n",
       " 'Oracle Park': 'San Francisco',\n",
       " 'Fort Bragg Field baseball': 'Fort Bragg',\n",
       " 'Williamsport, Pennsylvania': 'Williamsport'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOT USED \n",
    "\n",
    "## this is a code to get back city names from geolocations \n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "# geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# interim_dict = {}\n",
    "# city_dict = {}\n",
    "\n",
    "# for k,v in location_dict.items(): \n",
    "#     interim_dict[k]=((location_dict[k].latitude,location_dict[k].longitude))\n",
    "    \n",
    "# for k,v in interim_dict.items(): \n",
    "#     city_dict[k] = geolocator.reverse(interim_dict[k]).raw['address'].get('city','')\n",
    "    \n",
    "# city_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is not used \n",
    "\n",
    "# # doing one sample request from the API \n",
    "\n",
    "# BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/'\n",
    "# hist_forecast = 'history/'\n",
    "\n",
    "# latitude = 41.9481225\n",
    "# longitude = -87.6563513145702\n",
    "# hours = 24\n",
    "# start_date = '2019-06-13'\n",
    "# end_date = '2019-06-13'\n",
    "# json_or_csv = 'json'\n",
    "# api_key = 'LB75LJ6SXR4P5SP6U5HA666V6'\n",
    "\n",
    "# # specifying API parameters \n",
    "# locations = '?locations='+ str(latitude) + ',' + str(longitude)\n",
    "# aggregateHours = 'aggregateHours=' + str(hours)\n",
    "# startDateTime = 'startDateTime=' + str(start_date) + 'T00:00:00' \n",
    "# endDateTime = 'endDateTime=' + str(end_date) + 'T00:00:00'\n",
    "# unitGroup = 'unitGroup='+'uk'\n",
    "# contentType = 'contentType='+json_or_csv\n",
    "# dayStartTime = 'dayStartTime='+'0:0:00'\n",
    "# dayEndTime = 'dayEndTime='+'0:0:00'\n",
    "# key = 'key=' + str(api_key)\n",
    "\n",
    "# # create request link \n",
    "\n",
    "\n",
    "# res = '&'.join([locations,aggregateHours,startDateTime,endDateTime,unitGroup,contentType,dayStartTime,dayEndTime,key])\n",
    "\n",
    "# request_link = BaseURL+hist_forecast + res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
